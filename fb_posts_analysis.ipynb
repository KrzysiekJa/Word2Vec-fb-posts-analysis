{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00975437",
   "metadata": {},
   "source": [
    "<center><h1>Projekt</h1></center>\n",
    "<center><h1>Analiza wpisów profilu facebook'owego portalu Nowy Ład z wykorzystaniem algorytmu Word2Vec.</h1></center>\n",
    "<h3>Autor: Krzysztof Jarek</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048305ff",
   "metadata": {},
   "source": [
    "<h2><br>Wstęp teoretyczny.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d35593",
   "metadata": {},
   "source": [
    "Word2Vec jest to algorytm, czy bardziej precyzyjnie grupa metod, wykorzystywany w przetwarzaniu języka naturalnego (NLP). Zaliczny jest do szerszej grupy technik tzw. osadzania słów, inaczej opartych o <i>word embedding</i>, podejścia polegającego na reprezentowaniu, zapisywaniu słów za pomocą wektorów, które następnie jako struktury matematyczne mogą być wykorzystywane dla celów dalszej analizy, obliczeń [1].<br><br>\n",
    "Samo wspomniane <i>word embedding</i> w swojej idei opiera się o założenie przekształcenia tekstu na liczby (ustrukturyzowane zgrupowania liczb, np. wektory), przy czym dla tego samego tekstu mogą istnieć różne reprezentacje liczbowe. Wybór tego rozwiązania wynika z faktu, że wiele algorytmów uczenia maszynowego i generalnie wszystkie głębokiego uczenia nie są w stanie przetwarzać ciągów znaków ani zwykłego surowego tekstu. Z tego powodu potrzebują reprezentacji liczbowych jako danych wejściowych do wykonania dowolnego rodzaju przetwarzania (klasyfikacji, regresji). Można wyróżnić dwa rodzaje <i>word embedding</i>: oparte na częstotliwości (np. TF-IDF) i oparte na predykcji [2].<br><br>\n",
    "Wspomniane reprezentacje wektorowe, lub te oparte na częstotliwości, w generowanych za pomocą algorytmów modelach bazują na macierzach współwystępowania (<i>co-occurrence matrix</i>), które są sposobami przedstawiania jak często konkretne słowa współwystępują. Dwoma najpopularniejszymi typami macierzy są: macierze terminów-dokumentów i macierze terminów-terminów. Padające określenie współwystępowania jest tu rozumiane w ten sposób, że w przypadku danego korpusu słów przy pojawianiu się danej pary słów w zdefiniowanym oknie kontekstowym odpowiada liczba wyrażajaca w ten sposób matematyczne podobieństwo (<i>similarity</i>) tych słów [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b029cf1",
   "metadata": {},
   "source": [
    "<center><img src=\"images/word_embedding.png\" width=650 height=650 /></center>\n",
    "<center>Rys. 1 Przedstawienie idei okna kontesktowego dla wektorowego <i>word embedding</i> [3].</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a2672",
   "metadata": {},
   "source": [
    "W ramach domeny NLP jako metryka dla mierzenia wspomnianego podobieństwa (<i>similarity</i>) słów reprezentowanych przez wektory z przyczyn praktycznych przyjęła się metryka cosinusowa. Jej wzór prezentuje się następująco:<br>\n",
    "<center><img src=\"images/cosine_metric.png\" width=450 height=450 /></center>\n",
    "<center>gdzie: <b>v, w</b> to wektory [4].</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a9e75",
   "metadata": {},
   "source": [
    "Wracając do samej metody Word2Vec- jej wykorzystanie w celu stworzenia modelu służącego do rekonstrukcji językowych kontekstów słów opiera się o to, że przygotowany korpus słów jest wykorzytywany w ramach procesu uczenia dwuwarstwowych sieci neuronowe, który to proces ma ostatecznie doprowadzić do wygenerowania przestrzeni wektorowej w jakiej każdemu słowu ze zbioru wejściowego przypisywany jest odpowiedni wektor (w tak uzyskanej przestrzeni wektorowej). Jeśli chodzi o metodę to istotne jest to, iż pozwala ona na to, że słowa powiązane ze sobą kontekstem mają wektory znajdujące się blisko siebie w przestrzeni wyjściowej [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456cd59",
   "metadata": {},
   "source": [
    "<center><img src=\"images/example.png\" width=550 height=550 /></center>\n",
    "<center>Rys. 2 Przykład przestrzeni wyjściowej dla Word2Vec [6].</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571325b",
   "metadata": {},
   "source": [
    "Ze względu na architekturę wyróżnia się dwa główne podejścia Word2Vec, gdzie decydującą cechą jest charakter dokonywania rekonstrukcji językowych kontekstów słów:\n",
    "<ul>\n",
    "    <li>CBOW (<i>continuous bag-of-words</i>)- jakie opiera się o założenie, że na podstawie słów tworzących kontekst dokonuje się predykcji słowa, które w takim kontekście powinno się znajdować,</li>\n",
    "    <li>Skip-gram- odwrotnie, dla jakiego na podstawie słowa dokonuje się predykcji kontekstu składającego się z otaczających słów [5].</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd04ec4",
   "metadata": {},
   "source": [
    "<center><img src=\"images/architectures.png\" width=600 height=600 /></center>\n",
    "<center>Rys. 3 Zasada działania architektur CBOW i Skip-gram [5].</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14bed6e",
   "metadata": {},
   "source": [
    "Jeśli chodzi o funkcję uczenia (funkcję celu) dla modeli Word2Vec to można wymienić jej specyficzne stosowane odmiany. Pierwotnie stosowana była po prostu funkcja <i>softmax</i> przyjmująca wektory reprezentujące słowa, ale z racji na obciążenie obliczeniowe została zastąpiona przez następujące:\n",
    "<ul>\n",
    "    <li>hierarchiczny softmax- gdzie liczba obliczeń jest redukowana poprzez wykorzystanie drzewa binarnego (np. drzewo Huffman'a) za sprawą użycia jakiego tylko część węzłów w sieci neuronowej musi być wykorzystana dla danego etapu obliczeń ( $log_{2}$<i>liczba węzłów</i> ),</li>\n",
    "    <li>negatywne próbkowanie- rozwiązuje problem maksymalizacji prawdopodobieństwa poprzez minimalizację prawdopodobieństwa (logarytmicznego) próbkowanych wystąpień negatywnych [7].</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6991b",
   "metadata": {},
   "source": [
    "<h2><br><br><br>Część praktyczna.</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853fe1f",
   "metadata": {},
   "source": [
    "W tej części projektu przeszedłem do praktycznego wykorzystania wcześniej omówionej metody. Jako zbiór uczący dla przeprowadzenia analizy przygotowałem korpus składający się z 95 wpisów z serwisu facebook profilu portalu Nowy Ład. Profil ten wybrałem z racji na sam charakter wpisów: generalnie nie lakonicznych (jak np. profile Christianitas, OkoPress) czy zawierających tekst jako taki bez licznych linków, treści promocyjnych.<br><br>\n",
    "Do samej analizy wykorzystałem technologię języka python3 i narzędzia dostarczone przez pakiety takie jak <i>gensim</i> (Word2Vec), <i>nltk</i>, <i>re</i>, <i>pandas</i>. Pracę zacząłem od działań takich jak wczytanie przygotowanych danych, poglądowe ich wyświetlenie i wstępne przetworzenie do postaci zdatnej do poddania ich procesowi uczenia. Wszytko wymienione zostało umieszczone przeze mnie poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877938dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5da193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/fb_posts.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc85e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba wykorzystanych wpisów z facebook'a: 95\n"
     ]
    }
   ],
   "source": [
    "print('Liczba wykorzystanych wpisów z facebook\\'a:', df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b916e6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13-07-2022</td>\n",
       "      <td>Erę rządów Abe można ocenić pozytywnie głównie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13-07-2022</td>\n",
       "      <td>Pierwsze dni lipca upłynęły w Uzbekistanie pod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13-07-2022</td>\n",
       "      <td>Od początku działalności naszej młodej inicjat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13-07-2022</td>\n",
       "      <td>Zapraszamy do dołączenia do grupy sympatyków p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12-07-2022</td>\n",
       "      <td>Coraz konsekwentniej realizowana przez ustawod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               Post\n",
       "0  13-07-2022  Erę rządów Abe można ocenić pozytywnie głównie...\n",
       "1  13-07-2022  Pierwsze dni lipca upłynęły w Uzbekistanie pod...\n",
       "2  13-07-2022  Od początku działalności naszej młodej inicjat...\n",
       "3  13-07-2022  Zapraszamy do dołączenia do grupy sympatyków p...\n",
       "4  12-07-2022  Coraz konsekwentniej realizowana przez ustawod..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8eb58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>14-06-2022</td>\n",
       "      <td>Zagrożenia ekologiczne są jednym z kluczowych ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>14-06-2022</td>\n",
       "      <td>Czy rosyjskie elity trwale zwrócą się przeciwk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>13-06-2022</td>\n",
       "      <td>Wobec największego po 1989 r kryzysu w naszym ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>13-06-2022</td>\n",
       "      <td>Trwająca od ponad 100 dni wojna na Ukrainie po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>12-06-2022</td>\n",
       "      <td>Jakie są historyczne i ideowe źródła tożsamośc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date                                               Post\n",
       "90  14-06-2022  Zagrożenia ekologiczne są jednym z kluczowych ...\n",
       "91  14-06-2022  Czy rosyjskie elity trwale zwrócą się przeciwk...\n",
       "92  13-06-2022  Wobec największego po 1989 r kryzysu w naszym ...\n",
       "93  13-06-2022  Trwająca od ponad 100 dni wojna na Ukrainie po...\n",
       "94  12-06-2022  Jakie są historyczne i ideowe źródła tożsamośc..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55424910",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_posts = df.Post.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9f3e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = []\n",
    "[corpora.extend(sent_tokenize( re.sub(r'[„”\"@’]', '', text) )) for text in all_posts];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a3299f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba zdań składających się na korpus: 408 .\n"
     ]
    }
   ],
   "source": [
    "print('Liczba zdań składających się na korpus:', len(corpora), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ce2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(sentence) for sentence in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a052f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a47ed4",
   "metadata": {},
   "source": [
    "Tekst jako taki poddałęm procesowi oczyszczenia ze znaków, które nie wnosiłyby zbyt wiele dla przeprowadzanej analizy, a mogłyby np. obniżyć rezultaty przeprowadzanego procesu uczenia (takich jak cydzysłów itd.). Dalej, korpus podzieliłem, wyodrębniając poszczególne zdania. Mając już dostępne same zdania, przetransformowałem je do postaci list składających się z ciągami znaków:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed2899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['erę', 'rządów', 'abe', 'można', 'ocenić', 'pozytywnie', 'głównie', 'z', 'powodu', 'swoistego', 'odrodzenia', 'narodowego', 'japonii']\n",
      "['państwo', 'ponownie', 'rozbudziło', 'wśród', 'społeczeństwa', 'uczucia', 'patriotyczne', 'między', 'innymi', 'poprzez', 'prowadzenie', 'odpowiedniej', 'polityki', 'edukacyjnej', 'wśród', 'najmłodszej', 'części', 'populacji']\n",
      "['imponujący', 'jest', 'także', 'rozwój', 'japońskich', 'sił', 'samoobrony', 'które', 'także', 'pod', 'obecnymi', 'rządami', 'mogą', 'liczyć', 'na', 'coraz', 'większe', 'fundusze']\n",
      "['z', 'drugiej', 'strony', 'zamordowany', 'były', 'premier', 'nie', 'poradził', 'sobie', 'na', 'polu', 'gospodarczym', 'nie', 'odwracając', 'negatywnych', 'trendów', 'ostatnich', 'trzech', 'straconych', 'dekad']\n",
      "['i', 'tak', 'spuścizna', 'abe', 'pozostanie', 'jednak', 'na', 'bardzo', 'długo', 'nie', 'tylko', 'z', 'powodu', 'jego', 'tragicznej']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "666fdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4277d",
   "metadata": {},
   "source": [
    "Proces uczenia przeprowadziłem bezpośrednio uruchamijąc go wraz z utworzeniem obiektu dostarczonej klasy Word2Vec (domyślna działanie konstruktora). Wyjaśnienie parametrów konstruktora klasy:\n",
    "<ul>\n",
    "    <li>sentences - dostarczony zbiór danych treningowych,</li>\n",
    "    <li>sg - jeśli 1 to realizowany jest Skip-gram (inaczej CBOW),</li>\n",
    "    <li>window - liczba słów tworzących okno kontekstowe,</li>\n",
    "    <li>min_count - minimalna liczba zliczeń dla słowa tak by zostało dołączone do wyjściowego słownika,</li>\n",
    "    <li>epochs - liczba epok uczenia sieci,</li>\n",
    "    <li>workers - liczba wątków na jakie zostanie rozłożone wykonanie procesu.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbcef85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences = sentences, \n",
    "    sg = 1, # skip-gram\n",
    "    window = 5, \n",
    "    min_count = 1, \n",
    "    epochs = 8, \n",
    "    workers = Pool()._processes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fff3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34164977",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d62ac3",
   "metadata": {},
   "source": [
    "Następnie przeszedłem do samej analizy. Pakiet <i>gensim</i> dostarcza metodę obiektu typu Word2Vec <i>most_similar()</i>, która zwraca <i>n</i> słów, które mają najwyższy współczynnik podobieństwa względem słowa podanego na wejściu. Poniżej zaprezentowałem szereg wywołań tej metody z wybranymi słowami, jakie zwróciły następujące rezultaty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8deab091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 0.9666223526000977),\n",
       " ('na', 0.9662830829620361),\n",
       " ('do', 0.9661742448806763),\n",
       " ('dla', 0.965888500213623),\n",
       " ('o', 0.9655967354774475),\n",
       " ('polski', 0.9655476808547974),\n",
       " ('przez', 0.965462863445282),\n",
       " ('ii', 0.965330183506012),\n",
       " ('się', 0.9652944803237915),\n",
       " ('z', 0.9650025963783264)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('polska')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "912acc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polski', 0.9506455659866333),\n",
       " ('dr', 0.9505966901779175),\n",
       " ('ukrainy', 0.9479331970214844),\n",
       " ('jak', 0.9475066661834717),\n",
       " ('i', 0.9464317560195923),\n",
       " ('się', 0.946362316608429),\n",
       " ('dla', 0.9463019967079163),\n",
       " ('z', 0.9461473226547241),\n",
       " ('jako', 0.9461274743080139),\n",
       " ('to', 0.9461025595664978)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('turcja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18d82ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('utrata', 0.6542149186134338),\n",
       " ('zachodnią', 0.6423894166946411),\n",
       " ('wysokiej', 0.6416317224502563),\n",
       " ('wojnę', 0.6303442120552063),\n",
       " ('kontynuują', 0.6283208131790161),\n",
       " ('uam', 0.6236923336982727),\n",
       " ('zwiększyć', 0.6207494139671326),\n",
       " ('zrobić', 0.6182249188423157),\n",
       " ('biblioteki', 0.6180039048194885),\n",
       " ('aby', 0.6174520254135132)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('afryki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee953d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dla', 0.9743965864181519),\n",
       " ('nie', 0.9740657210350037),\n",
       " ('do', 0.9736761450767517),\n",
       " ('tylko', 0.9736183285713196),\n",
       " ('w', 0.9734273552894592),\n",
       " ('a', 0.97282475233078),\n",
       " ('o', 0.972655713558197),\n",
       " ('to', 0.9726532101631165),\n",
       " ('być', 0.9725211262702942),\n",
       " ('za', 0.9724376797676086)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('ukraina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7209f744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 0.9816751480102539),\n",
       " ('i', 0.9816627502441406),\n",
       " ('jest', 0.9814104437828064),\n",
       " ('w', 0.9808385372161865),\n",
       " ('czy', 0.9807396531105042),\n",
       " ('się', 0.9802049398422241),\n",
       " ('od', 0.9801687002182007),\n",
       " ('o', 0.9801136255264282),\n",
       " ('co', 0.980099081993103),\n",
       " ('polski', 0.9800916314125061)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('usa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "073c26a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tylko', 0.9455297589302063),\n",
       " ('ze', 0.9452150464057922),\n",
       " ('jest', 0.9439623951911926),\n",
       " ('można', 0.9426714777946472),\n",
       " ('są', 0.9424187541007996),\n",
       " ('a', 0.942260205745697),\n",
       " ('najbardziej', 0.9418258666992188),\n",
       " ('o', 0.940778374671936),\n",
       " ('od', 0.9407326579093933),\n",
       " ('po', 0.9407224655151367)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('premier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "827d19fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('albo', 0.7203791737556458),\n",
       " ('stwarzają', 0.7122724652290344),\n",
       " ('samodzielną', 0.7062618732452393),\n",
       " ('kierunku', 0.7062522768974304),\n",
       " ('kity', 0.704880952835083),\n",
       " ('dołączenia', 0.7046069502830505),\n",
       " ('większość', 0.7036066055297852),\n",
       " ('wynik', 0.7019095420837402),\n",
       " ('ukrainie', 0.7018398642539978),\n",
       " ('ilości', 0.701668381690979)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('warszawa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96132b",
   "metadata": {},
   "source": [
    "Dla przyjrzenia się ściślej problemowi preferowanej w NLP metryki cosinusowej, przygotowałem specjalną implementację dla przebadania tego jakie wartości mogą zostać zwrócone dla przypadków wybranych par słów. Implementację wraz z przykładami zamieściłęm poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79562336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0e023bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(wv0, wv1):\n",
    "    return 1 - spatial.distance.cosine(wv0, wv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5894d2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5165625810623169"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv0 = model.wv['warszawa']\n",
    "wv1 = model.wv['miasto']\n",
    "cosine_similarity(wv0, wv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a61a176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111180901527405"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv0 = model.wv['politycy']\n",
    "wv1 = model.wv['polityki']\n",
    "cosine_similarity(wv0, wv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e7ae7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9476596117019653"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv0 = model.wv['polska']\n",
    "wv1 = model.wv['ukraina']\n",
    "cosine_similarity(wv0, wv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b65e003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9322970509529114"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv0 = model.wv['ukraina']\n",
    "wv1 = model.wv['francja']\n",
    "cosine_similarity(wv0, wv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c439d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9177106022834778"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv0 = model.wv['ukraina']\n",
    "wv1 = model.wv['rosji']\n",
    "cosine_similarity(wv0, wv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27a18d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9161891341209412"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv0 = model.wv['ukraina']\n",
    "wv1 = model.wv['turcja']\n",
    "cosine_similarity(wv0, wv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5896b",
   "metadata": {},
   "source": [
    "Na koniec ponownie podjąłem się wykorzystania metody <i>most_similar()</i>- tym razem wykorzystując możliwość pooperowania na wbudowanych parametrach <i>positive</i> i <i>negative</i>, które to pozwalają nie tylko na pozyskiwanie probabilistycznych wyników zwracanych przez model dla wybranych słów czy zbiorów słów, ale co więcej pozwalają na przeprowadzanie operacji algebraicznych. Operacje te faktycznie są przeprowadzane jako operacje na wektorach reprezentujących słowa- ta oto idea możliwości przeprowadzania operacji algebraicznych była celem, który środowisko zajmujące się NLP bardzo chciało osiągnąć. Przykładowe operacje zamieściłem poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3641a2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('albo', 0.6754295825958252), ('konserwatywny', 0.6609734296798706)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['warszawa','turcja'],negative=['polska'],topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25ec1f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polacy', 0.9354678988456726)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['francja','bez','kitu'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba83fe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kryzysu', 0.6659268140792847),\n",
       " ('stary', 0.6442970037460327),\n",
       " ('system', 0.6421110033988953)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['facebooka','polityka'],negative=['google'],topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff928e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wobec', 0.9582085609436035), ('polski', 0.9562836289405823)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['henry','kissinger','powiedział'],topn=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818f988",
   "metadata": {},
   "source": [
    "<h2>Wnioski:</h2>\n",
    "<ul>\n",
    "    <li>Word2Vec jest metodą pozwalającą dokonywać w sposób bardzo wyrafinowany rekonstruowania językowych kontekstów związanych z konkretnymi słowami, zgrupowaniami słów. Co więcej pozwala ona zachowywać liniowe zależności występujące między słowami (reprezentowanymi przez wektory) w ich oryginalnych kontekstach lingwistycznych.</li>\n",
    "    <li>Word2Vec pozwala na posługiwanie się obiektami wyjściowej przestrzeni wektorowej w sposób bardziej zaawansowany, dostarczając możliwości wykorzystywania operacji algebraicznych <i>de facto</i> odnoszących się bezpośrednio do samych słów. Co jest dużym rozwinięciem analitycznym i technologicznym dla domeny NLP.</li>\n",
    "    <li>By rezultaty zwracane prez Word2Vec były satysfakcjonujące z perspektywy przeprowadzania procesu uczenia, wskazane jest by przygotowywać odpowiednio duże zbiory tekstowe (przygotowany dla badania mógł być niedostatecznie duży).</li>\n",
    "    <li>Na podstawie samej przeprowadzonej analizy można zauważyć, że w przebadanych kontekstach słów leksy słów \"polski\" i \"Ukraina\" często pojawiały się na listach słów podobnych do badanych. Poza tym dominowały zdecydowanie spójniki.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95212ba1",
   "metadata": {},
   "source": [
    "<h2>Przypisy:</h2>\n",
    "\n",
    "[1] D. Jurafsky, H. J. Martin, \"Vector Semantics and Embeddings\", s. 17-18, [w:] <i>Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition.</i>, 2021, [na:] https://web.stanford.edu/~jurafsky/slp3/, [dostęp:] 17.07.2022.<br>\n",
    "[2] <i>Ibidem</i>, s. 5-10.<br>\n",
    "[3] J. Ramkissoon, \"First Steps with Word Embeddings\", 2019, [na:] https://jramkiss.github.io/2019/08/21/word-embeddings/, [dostęp:] 17.07.2022.<br>\n",
    "[4] D. Jurafsky, H. J. Martin, \"Vector Semantics...\", s. 10-11.<br>\n",
    "[5] T. Mikolov (red.), \"Efficient Estimation of Word Representations in Vector Space\", 2013, s. 4-5.<br>\n",
    "[6] T. Mikolov, W. Yih, G. Zweig, \"Linguistic Regularities in Continuous Space Word Representations\", 2013, s. 4.<br>\n",
    "[7] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, \"Distributed representations of words and phrases and their compositionality\", 2013, s. 2-4.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
